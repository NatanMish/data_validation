{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Database data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook, we will see how we can use the great_expectations package to validate data in our database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NatanMish/data_validation/blob/main/notebooks/database_data_validation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Install the required packages and import them to the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U great_expectations pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "import great_expectations as ge\n",
    "from ruamel import yaml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[![Great Expectations](https://docs.greatexpectations.io/img/great-expectations-long-logo.svg)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Great Expectations is a shared, open sourced package for data quality. It helps eliminate pipeline debt, through data testing, documentation, and profiling. It is a tool for data scientists, data engineers, and data analysts to validate data. GE has many useful integrations and can be connected directly to SQL databases, Apache Spark, Apache Airflow, Bigquery, and more. In this tutorial, we will validate a database hosted on a  local file system, but the process for a cloud file system such as a Data Lake, Azure Blob Storage, GCP bucket or AWS S3 is almost identical.\n",
    "\n",
    "**Terminology**\n",
    "1. *Data Context* - The primary entry point for a Great Expectations deployment, with configurations and methods for all supporting components.\n",
    "\n",
    "2. *Data Source* - Provides a standard API for accessing and interacting with data from a wide variety of source systems.\n",
    "\n",
    "3. *Data Asset* - A collection of records within a Datasource which is usually named based on the underlying data system and sliced to correspond to a desired specification.\n",
    "\n",
    "4. *Expectation Suite* - A collection of verifiable assertions about data.\n",
    "\n",
    "5. *Validation* - The act of applying an Expectation Suite to a Batch.\n",
    "\n",
    "6. *Batch Identifier* - contains information that uniquely identifies a specific batch from the Data Asset, such as the delivery date or query time.\n",
    "\n",
    "7. *Data Connector* - Provides the configuration details based on the source data system which are needed by a Datasource to define Data Assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Create a Data Context\n",
    "\n",
    "We will now create a data context, which is the first step in setting up Great Expectations for our project. Creating a data context is actually most easily done in bash using the great_expectations CLI. Run the bash command below and this will initialize a new data context in the current directory. The `echo y` bit is used to suppress the interactive prompt. You will now see a new directory called `great_expectations` created in your current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!echo y | great_expectations init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After running the init command, your great_expectations directory will contain all the important components of a local Great Expectations deployment. This is what the directory structure looks like:\n",
    "\n",
    "- `great_expectations.yml` contains the main configuration of your deployment.\n",
    "The expectations directory stores all your Expectations as JSON files. If you want to store them somewhere else, you can change that later.\n",
    "\n",
    "- The `plugins/` directory holds code for any custom plugins you develop as part of your deployment.\n",
    "\n",
    "- The `uncommitted/` directory contains files that shouldnâ€™t live in version control. It has a .gitignore configured to exclude all its contents from version control. The main contents of the directory are:\n",
    "    1. `uncommitted/config_variables.yml`, which holds sensitive information, such as database credentials and other secrets.\n",
    "    2. `uncommitted/data_docs`, which contains Data Docs generated from Expectations, Validation Results, and other metadata.\n",
    "    3. `uncommitted/validations`, which holds Validation Results generated by Great Expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "<img src=\"https://docs.greatexpectations.io/assets/images/data_context_does_for_you-df2eca32d0152ead16cccd5d3d226abb.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Create a Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We will start by reading in the GE data context we have created in the previous step\n",
    "context = ge.get_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we will script a yaml file to create a data source. We will need the following configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasource_name = \"house_prices\"\n",
    "# Data Source - Provides a standard API for accessing and interacting with data from a wide variety of source systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "execution_engine = \"PandasExecutionEngine\"  # alternatively we can use SparkExecutionEngine for PySpark oriented\n",
    "# projects or SqlAlchemyExecutionEngine for creating a SQL database data source.\n",
    "data_directory = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_asset_name = f\"{datasource_name}_survey_2006\"\n",
    "# Data Asset - A collection of records within a Datasource which is usually named based on the underlying data system and sliced to correspond to a desired specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "runtime_data_connector_name = \"runtime_batch_files_connector\"\n",
    "# Data Connector - Provides the configuration details based on the source data system which are needed by a Datasource to define Data Assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasource_config = {\n",
    "    \"name\": datasource_name,\n",
    "    \"class_name\": \"Datasource\",\n",
    "    \"module_name\": \"great_expectations.datasource\",\n",
    "    \"execution_engine\": {\n",
    "        \"module_name\": \"great_expectations.execution_engine\",\n",
    "        \"class_name\": execution_engine,\n",
    "    },\n",
    "    \"data_connectors\": {\n",
    "        runtime_data_connector_name: {\n",
    "            \"class_name\": \"RuntimeDataConnector\",\n",
    "            \"batch_identifiers\": [\"default_identifier_name\"],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test that the configuration is valid\n",
    "context.test_yaml_config(yaml.dump(datasource_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If the configuration is valid, we can create the datasource\n",
    "context.add_datasource(**datasource_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now we can see that the datasource was created.\n",
    "context.list_datasources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Create an Expectation Suite\n",
    "Expectations are the core of Great Expectations. They are the assertions that are used to validate data. Let's create an expectation suite which is a collection of expectations. This diagram below shows how we can define good expectations for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "<img src=\"https://docs.greatexpectations.io/assets/images/where_expectations_come_from-b3504cf51ad304c8e4a73677a0e73156.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will create expectations while exploring the data in the notebook. The method below behaves  exactly the same as `pandas.read_csv`. Similarly wrapped versions of other pandas methods (`read_excel`, `read_table`, `read_parquet`, `read_pickle`, `read_json`, etc.) are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "home_data = ge.read_csv(\"https://github.com/NatanMish/data_validation/blob/a77b247b25c6622ce0c8f8cbc505228161c31a3c/data/train.csv?raw=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The home_data variable is a pandas dataframe with all the methods and properties we know and love. We can use the `head` method to see the\n",
    "# first few rows of the data.\n",
    "home_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# beyond the Pandas methods and properties, we can use GE's expectations methods to define expectations. \n",
    "# In Jupyter, type in `home_data.expect` and press tab to see the list of available expectations.\n",
    "home_data.expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a few example expectations and see if they are valid on this dataset.\n",
    "home_data.expect_column_to_exist(\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice the `\"success\": true` key in the result dictionary, this means the expectation is valid for this data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "home_data.expect_column_values_to_be_unique(\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The expectation above checked the contents of the column, hence we got a few other useful metrics, showing how many \n",
    "rows were inspected, how many were missing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This expectation should fail, lets see what happens:\n",
    "home_data.expect_column_max_to_be_between(\"SalePrice\", 0, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The returned dictionary shows that the expectation is not valid, and the value observed that is not in the expected range.\n",
    "Here are a few more useful expectation definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% mdhhome_data.expect_column_distinct_values_to_be_in_set\n"
    }
   },
   "outputs": [],
   "source": [
    "home_data.expect_column_distinct_values_to_be_in_set(\"MSZoning\", [\"C (all)\", \"FV\", \"RH\", \"RL\", \"RM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "home_data.expect_column_mean_to_be_between(\"GrLivArea\", 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This will create an expectation suite from all the valid expectations we created above.\n",
    "home_data.get_expectation_suite()\n",
    "# If we want the non-valid expectations as well, we can use the `get_expectation_suite` method with the \n",
    "# `discard_failed_expectations` parameter set to True. If there are any duplicate expectations in the suite, \n",
    "# the duplicates will be discarded:\n",
    "# home_data.get_expectation_suite(discard_failed_expectations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This line will save the expectation suite to the data context\n",
    "context.save_expectation_suite(home_data.get_expectation_suite(), \"my_expectations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 1\n",
    "Check the following expectations to see if they are valid on the home_data dataframe:\n",
    "\n",
    "(Not all the expectations were included in the examples above. You can find more expectations in the [expectations directory](https://greatexpectations.io/expectations).)\n",
    "1. `Street` column should be a string.\n",
    "2. `LandContour` column cannot be null.\n",
    "3. `YearBuilt` minimal value should be between 1700 and 1900.\n",
    "4. `LotArea` median value should be between 5000 and 15000.\n",
    "5. The most common values in `SaleType` must be either `WD` or `New`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your answers here:\n",
    "# home_data.expect_column_\n",
    "# home_data.expect_column_\n",
    "# home_data.expect_column_\n",
    "# home_data.expect_column_\n",
    "# home_data.expect_column_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Exercise solutions can be found in the exercise solutions file in the current directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. Validate the Data\n",
    "We will now validate the test data using the expectations we have created for the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_name = \"data_batch_appended\"\n",
    "# Checkpoint - The primary means for validating data in a production deployment of Great Expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_config = {\n",
    "    \"name\": checkpoint_name,\n",
    "    \"config_version\": 1,\n",
    "    \"class_name\": \"SimpleCheckpoint\",\n",
    "    \"validations\": [\n",
    "        {\n",
    "            \"batch_request\": {\n",
    "                \"datasource_name\": datasource_name,\n",
    "                \"data_connector_name\": runtime_data_connector_name,\n",
    "                \"data_asset_name\": data_asset_name,\n",
    "            },\n",
    "            \"expectation_suite_name\": \"my_expectations\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "context.add_checkpoint(**checkpoint_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at the dictionary returned by the `add_checkpoint` methods we can see what are the actions performed every time the checkpoint will run:\n",
    "1. Store validation result.\n",
    "2. Store evaluation parameters.\n",
    "3. Update data docs. (we will look at the data docs later in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "home_data_test = pd.read_csv(\"https://github.com/NatanMish/data_validation/blob/a77b247b25c6622ce0c8f8cbc505228161c31a3c/data/test.csv?raw=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = context.run_checkpoint(\n",
    "    checkpoint_name=checkpoint_name,\n",
    "    batch_request={\n",
    "        \"runtime_parameters\": {\"batch_data\": home_data_test},\n",
    "        \"batch_identifiers\": {\n",
    "            \"default_identifier_name\": \"default_identifier_name\"\n",
    "        },\n",
    "    },\n",
    ")\n",
    "# Batch Identifier - contains information that uniquely identifies a specific batch from the Data Asset, such as the delivery date or query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the validation result object we got:\n",
    "run_identifier = next(iter(results['run_results']))\n",
    "results['run_results'][run_identifier]['validation_result']['statistics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Here is an example of one of the validations on one of the expectations. The check has passed and there are some \n",
    "# useful extra details too.\n",
    "results['run_results'][run_identifier]['validation_result']['results'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### How does an invalid data checkpoint look like?\n",
    "Glad you asked, let's inject a null value to our `Id` column to see how it behaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This will create a duplicate id value for two separate records\n",
    "home_data_test.at[0, 'Id'] = 1462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bad_data_checkpoint_name = \"my_bad_data_checkpoint\"\n",
    "bad_data_checkpoint_config = {\n",
    "    \"name\": bad_data_checkpoint_name,\n",
    "    \"config_version\": 1,\n",
    "    \"class_name\": \"SimpleCheckpoint\",\n",
    "    \"validations\": [\n",
    "        {\n",
    "            \"batch_request\": {\n",
    "                \"datasource_name\": datasource_name,\n",
    "                \"data_connector_name\": runtime_data_connector_name,\n",
    "                \"data_asset_name\": \"batch_data_asset\",\n",
    "            },\n",
    "            \"expectation_suite_name\": \"my_expectations\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "context.add_checkpoint(**bad_data_checkpoint_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_bad_data_checkpoint = context.run_checkpoint(\n",
    "    checkpoint_name=bad_data_checkpoint_name,\n",
    "    batch_request={\n",
    "        \"runtime_parameters\": {\"batch_data\": home_data_test},\n",
    "        \"batch_identifiers\": {\n",
    "            \"default_identifier_name\": \"default_identifier_name\"\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# As expected, not all expectations were successful.\n",
    "bad_data_run_identifier = next(iter(results_bad_data_checkpoint['run_results']))\n",
    "results_bad_data_checkpoint['run_results'][bad_data_run_identifier]['validation_result']['statistics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# And here is the summary for the failed expectation\n",
    "results_bad_data_checkpoint['run_results'][bad_data_run_identifier]['validation_result']['results'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}